---
title: "Assignment1"
output: html_document
date: "2025-10-08"
---
```{r}
# Clear plots
if(!is.null(dev.list())) dev.off()

# Clean workspace
rm(list=ls())
```



# 1. Data preparation
```{r}
# Loading data
df <- read.csv("RealEstate_Georgia.csv", header=T)

# Convert date column to Date format (YYYY-MM-DD) and extract the year as numeric
df$datePostedString <- as.Date(df$datePostedString, format = "%Y-%m-%d")
df$datePostedString <- as.numeric(format(df$datePostedString, format = "%Y"))

# Count rows where year is 2021 and only keep these
length(df[which(df$datePostedString == 2021),]$datePostedString)
df <- df[which(df$datePostedString == 2021),]

# Duplicate quick inspection
dup_rows <- duplicated(df$id) | duplicated(df$id, fromLast = TRUE) # Identify duplicated IDs
dups_df  <- df[dup_rows, ][order(df$id[dup_rows]), ] # Create a df with only duplicated rows, sorted by ID
head(dups_df, 20) # Show first 20 duplicated entries

```
**Problem:** 
1) Some observations have the same information for all variables but different `countyId`. Since the variable `county` shows the same value for duplicated instances, `countyId` (5) will be removed due to the inconsistencies. If a numerical representation is needed, we will provide one. 

```{r}
# Columns to compare (exclude Ids and countyId) without dropping them from df
cols_idx <- setdiff(seq_along(df), c(1:3, 5))

## Comparison by id (which columns differ between duplicates)
by_id <- split(dups_df[, cols_idx, drop = FALSE], dups_df$id)

# Group duplicates by `id` and compare the selected columns
diffs_list <- lapply(by_id, function(sub) {
  difs <- vapply(sub, function(x) length(unique(x)) > 1, logical(1))
  names(difs)[difs]              # solo las columnas que cambian
})


# summary dataframe indicating differences by id
summary_df <- data.frame(
  id       = names(diffs_list),
  n_diff   = lengths(diffs_list),
  diff_vars = ifelse(lengths(diffs_list) == 0, "None",
                     vapply(diffs_list, function(x) paste(x, collapse = ", "), character(1))),
  stringsAsFactors = FALSE
)

# Frequency of how many duplicates have 0, 1, 2, etc. differing variables
table(summary_df$n_diff)

# Frequency of variable combinations that differ (e.g., "countyId", "countyId, price")
sort(table(summary_df$diff_vars), decreasing = TRUE)
```
**Problem:** 
We manage the duplicates separating them in 2 categories.

Case 1) Exact duplicates: duplicates with same information across all variables but different `countyId` (None group).
    - Action: select one of the two duplicates.
    
Case 2) Conflicting duplicates: some duplicates have different information for some variables
    - Action: check these carefully.

```{r}
# Case 1: Exact duplicates
ids_none <- summary_df$id[summary_df$diff_vars == "None"] 
# keep only the 1st occurrence
keep_idx <- ifelse(df$id %in% ids_none, !duplicated(df$id), TRUE) 
df <- df[keep_idx, ]

# check remaining duplicates among "None" group (should be 0)
sum(duplicated(df$id) & df$id %in% ids_none)  

```
```{r}
# Case 2: Duplicates with differences (require inspection)
summary_rest <- summary_df[summary_df$diff_vars != "None", ]

# Frequency of each difference combination (e.g., "price", "time, price")
sort(table(summary_rest$diff_vars), decreasing = TRUE)

## helper: function to see all rows for a given combination
see_comb <- function(combo_str) {
  ids <- summary_rest$id[summary_rest$diff_vars == combo_str]
  out <- df[df$id %in% ids, ]
  out[order(out$id), ]
}

# Example of use:
see_comb("time, price, pricePerSquareFoot") 
```
The helper function 'see_comb' allows us to watch the duplicates for the different combinations of differences they have (for example, observations with different description, different value for garageSpaces...)

After analyzing all duplicate groups, the conclusions we have reached are as follows:

  * Observations with different description: Since they have the same values for everything else, and the description is not a relevant variable, only one of the two duplicates will be retained.
  
  * Observations with combinations of different time+event, time+price: After checking all observations, it is confirmed that the duplicates correspond to price or event changes. The most recent observations are the ones to retain, since they contain the most recent information.
      ** One particular observation of this case also includes a modification in description, garageSpaces, hasGarage. Since the description hints that the Garage has been renovated to a basement, the most recent observation is also the one to include. 
  
  * One observation with different value for 'garageSpaces': since 'garage' has value 1 for both observations, we conclude the reliable description is the one where 'garageSpaces' is 1, not the one with value 0.
  
  * One observation with different value for 'parking': Since there is no reasonable way to find out which is the correct observation (value 0 or 1), the decision is to remove both duplicates, so as to not work with incorrect information.
  
  *One observation time, price, pricePerSquareFoot, description, garageSpaces, hasGarage"
  

### Duplicate removal
```{r}
## We start from:
## - df  (already deduped for "None" cases)
## - summary_rest = subset of summary_df where diff_vars != "None"

## ---- Identify ids by inconsistency type ----
ids_desc_only    <- unique(summary_rest$id[summary_rest$diff_vars == "description"])
ids_parking_only <- unique(summary_rest$id[summary_rest$diff_vars == "parking"])
ids_gs_only      <- unique(summary_rest$id[summary_rest$diff_vars == "garageSpaces"])

## IDs with any difference involving 'time' (e.g., time, price, event...) -> keep the row with the SMALLEST time (most recent)
ids_with_time    <- unique(summary_rest$id[grepl("\\btime\\b", summary_rest$diff_vars)])

keep_idx <- rep(TRUE, nrow(df)) # Initialize vector to mark rows to keep

## Rule A: "parking" only -> drop ALL rows for those ids (cannot decide which one is correct)
if (length(ids_parking_only)) {
  keep_idx[df$id %in% ids_parking_only] <- FALSE
}

## Rule B: "description" only -> keep just the first occurrence per id
if (length(ids_desc_only)) {
  w <- df$id %in% ids_desc_only
  keep_idx[w] <- !duplicated(df$id[w])
}

## Rule C: any combo that includes 'time' -> keep the row with the SMALLEST time (most recent)
## - If some times are NA, keep the smallest non-NA; if all are NA, keep the first row.
if (length(ids_with_time)) {
  for (idv in ids_with_time) {
    rows <- which(df$id == idv)
    if (length(rows) >= 2) {
      tvals <- suppressWarnings(as.numeric(df$time[rows]))
      if (all(is.na(tvals))) {
        keep <- rows[1]
      } else {
        # Replace NA with +Inf so they are never the minimum
        t2 <- ifelse(is.na(tvals), Inf, tvals)
        keep <- rows[which.min(t2)]
      }
      keep_idx[rows] <- FALSE
      keep_idx[keep] <- TRUE
    }
  }
}

## Rule D: "garageSpaces" only -> if hasGarage==1 exists, keep the row with garageSpaces==1;
## otherwise keep the first row.
if (length(ids_gs_only)) {
  for (idv in ids_gs_only) {
    rows <- which(df$id == idv)
    if (length(rows) >= 2) {
      gs <- df$garageSpaces[rows]
      hg <- df$hasGarage[rows]
      if (any(hg == 1, na.rm = TRUE) && any(gs == 1 & hg == 1, na.rm = TRUE)) {
        keep <- rows[which((gs == 1) & (hg == 1))[1]]
      } else {
        keep <- rows[1]
      }
      keep_idx[rows] <- FALSE
      keep_idx[keep] <- TRUE
    }
  }
}

df <- df[keep_idx, ]

any(duplicated(df$id))
```

```{r}
# ---- Clean up intermediate data frames ----
rm(list = setdiff(ls(), "df"))
gc()  # Optional: frees up memory

```

# 2. Data cleaning

## 2.1. Variable analysis

### Initial analysis
```{r}
summary(df)
```
#### Converting categorical variables to factors
```{r}
# Converting characters to factor
df[sapply(df, is.character)] <- lapply(df[sapply(df, is.character)], as.factor)

# Converting binary columns (only 0 or 1) to factor
df[sapply(df, function(x) all(na.omit(unique(x)) %in% c(0, 1)))] <- 
  lapply(df[sapply(df, function(x) all(na.omit(unique(x)) %in% c(0, 1)))], as.factor)

summary(df)
```
```{r}
summary(df$livingArea == df$livingAreaValue)
summary(df$livingArea == df$buildingArea)
```

Based on the data summary, several variables will be removed due to limited informational value or redundancy:

- `X.1`, `X`, `id`, and `time` will be removed as they do not contribute meaningful information for modeling.

- `stateId`, `state`, `country`, `currency`, and `datePostedString` ill be excluded because they show no variability; all records correspond to properties in Georgia (USA), are listed in USD, and fall within the same year.

- `hasBadGeocode` will be removed since it contains the same values, all zero.

- `is_bankOwned` and `is_forAuction` will be dropped due to extremely low variance (each contains only a single non-zero value), providing no practical value and potentially introducing noise.

- `livingAreaValue` and `buildingArea` are exact duplicates of `livingArea`, so they will be removed to avoid redundancy.

- `zipcode` will be excluded because location information is already captured by other variables (for example, city).

- `latitude` and `longitude` will be removed, as geographic coordinates are not relevant for the modeling objectives.

- `streetAddress` will also be removed, given that it uniquely identifies each property without offering generalizable information.


### 2.1.1. Target variable: `Price`
The first variable to be analyzed is Price, as it is a numerical variable representing the target of the analysis. 

The variable contains no missing values, so imputation is not required. Outlier detection identified 561 mild outliers and 32 extreme outliers, on the higher end of the spectrum. 

```{r}
summary(df$price)
sum(is.na(df$price))
```
```{r}
# Histogram + Normal curve
hist(df$price, breaks = 30, freq = F, main="Histogram of Price", col="cyan")
curve(dnorm(x, mean(df$price), sd(df$price)), add = T)
```
As can be seen in the histogram, the target variable `Price` does not follow a normal distribution. The distribution of `Price` is right-skewed, with most values concentrated between approximately 100,000 and 600,000, and a peak around 300,000. This indicates that lower-priced properties are more common, while higher-priced ones are less frequent but still present.

As the price increases, the frequency of observations declines progressively, and only a small number of properties exceed $1,000,000. This long right tail suggests the presence of high-value outliers, typical of luxury properties, we should take this into account as it could affect modeling.



!!!!!!!!!!!!!!!!!As it is not normal, should we transform it? log transformation


```{r}
# Normal distribution
ks.test(df$price, "pnorm", mean(df$price), sd(df$price))
```
The Kolmogorov-Smirnov test was performed to confirm whether the `price` variable follows a normal distribution. The test returned a p-value < 2.2e-16, which is statistically significant. Therefore, we reject the null hypothesis of normality and conclude that price does not follow a normal distribution.

We examine the outliers using the Interquartile Range (IQR) method:
```{r}
# Outliers

# Upper limits
varout <- summary(df$price); varout
iqr <- varout[5] - varout[2]; iqr # diff between IQR = Q3 - Q1
umout <- varout[5] + 1.5*iqr; umout # upper limit to identify mild outliers
usout <- varout[5] + 3*iqr; usout # upper limit to identify extreme outliers

# Lower limits
lmout <- varout[5] - 1.5*iqr; umout # upper limit to identify mild outliers
lsout <- varout[5] - 3*iqr; usout # upper limit to identify extreme outliers

boxplot(df$price, horizontal = T, id=list(labels=row.names(df), n=2))

abline(v=umout,col="orange",lwd=2) # vertical line for mild outliers
abline(v=usout,col="red",lwd=2.5) # vertical line for extreme outliers
abline(v=lmout,col="orange",lwd=2) # vertical line for mild outliers
abline(v=lsout,col="red",lwd=2.5) # vertical line for extreme outliers

mild_outliers_price <- which(df$price < lmout | df$price > umout)
extreme_outliers_price <- which(df$price < lsout | df$price > usout)

cat("Number of mild outliers:", length(mild_outliers_price), "\n")
cat("Number of extreme outliers:", length(extreme_outliers_price), "\n")
```
The `price` variable shows a substantial number of outliers, with 561 mild and 32 extreme cases, all located in the upper tail. These outliers might reflect the presence of high-value and luxury properties. Since these values carry meaningful market information, they will be retained. 

!!!!!!!!!!!!!!!!!!!!???????However, due to their potential influence on model performance, a logarithmic transformation will be considered to reduce skewness and stabilize variance.


### 2.1.2. Numerical variables
#### Variable `pricePerSquareFoot`
```{r}
# Summary statistics
summary(df$pricePerSquareFoot)
sum(is.na(df$pricePerSquareFoot))

# Histogram + Normal curve
hist(df$pricePerSquareFoot, breaks = 30, freq = FALSE, main = "Histogram of pricePerSquareFoot", xlab = "pricePerSquareFoot")
curve(dnorm(x, mean(df$pricePerSquareFoot, na.rm=TRUE), sd(df$pricePerSquareFoot, na.rm=TRUE)), add = TRUE, col = "red")
```
After examining the distribution of `pricePerSquareFoot`, the maximum value is disproportionately higher than the median, indicating the presence of extreme values or potential outliers. To investigate this further, we apply the Interquartile Range (IQR) method to detect and evaluate these outliers.

```{r}
# Outliers

# Upper limits
varout <- summary(df$pricePerSquareFoot); varout
iqr <- varout[5] - varout[2]; iqr # diff between IQR = Q3 - Q1
umout <- varout[5] + 1.5*iqr; umout # upper limit to identify mild outliers
usout <- varout[5] + 3*iqr; usout # upper limit to identify extreme outliers

# Lower limits
lmout <- varout[2] - 1.5*iqr; umout # upper limit to identify mild outliers
lsout <- varout[2] - 3*iqr; usout # upper limit to identify extreme outliers

boxplot(df$pricePerSquareFoot, horizontal = T, id = list(labels = rownames(df), n = 1), main = "Boxplot PricePerSquareFoot")

abline(v=umout,col="orange",lwd=2) # vertical line for mild outliers
abline(v=usout,col="red",lwd=2.5) # vertical line for extreme outliers

abline(v=lmout,col="orange",lwd=2) # vertical line for mild outliers
abline(v=lsout,col="red",lwd=2.5) # vertical line for extreme outliers

mild_outliers <- which(df$pricePerSquareFoot < lmout | df$pricePerSquareFoot > umout)
extreme_outliers <- which(df$pricePerSquareFoot < lsout | df$pricePerSquareFoot > usout)

cat("Number of mild outliers:", length(mild_outliers), "\n")
cat("Number of extreme outliers:", length(extreme_outliers), "\n")
```
The variable `pricePerSquareFoot` presents a strong right-skew with a median around 153, while the maximum reaches an unusually extreme value (over 200,000). The IQR analysis detects 258 mild outliers and 51 extreme outliers, confirming the presence of several abnormally high observations. These values likely result from data entry errors or exceptional listings and will require treatment to prevent them from disproportionately influencing the model.


```{r}
# Delete extreme outlier
df <- df[df$pricePerSquareFoot != 205000, ]
summary(df$pricePerSquareFoot)

# Histogram + Normal curve
hist(df$pricePerSquareFoot, breaks = 30, freq = FALSE, main = "Histogram of pricePerSquareFoot", xlab = "pricePerSquareFoot")
curve(dnorm(x, mean(df$pricePerSquareFoot, na.rm=TRUE), sd(df$pricePerSquareFoot, na.rm=TRUE)), add = TRUE, col = "red")
```
After removing the extreme outlier in 205000, the distribution of pricePerSquareFoot remains right-skewed but is now much more realistic and consistent, most observations fall between $100 and $200, with a median of 153 and a maximum around 1,143.

```{r}
# Outliers

#Upper limits
varout <- summary(df$pricePerSquareFoot); varout
iqr <- varout[5] - varout[2]; iqr # diff between IQR = Q3 - Q1
umout <- varout[5] + 1.5*iqr; umout # upper limit to identify mild outliers
usout <- varout[5] + 3*iqr; usout # upper limit to identify extreme outliers

# Lower limits
lmout <- varout[2] - 1.5*iqr; umout # upper limit to identify mild outliers
lsout <- varout[2] - 3*iqr; usout # upper limit to identify extreme outliers

boxplot(df$pricePerSquareFoot, horizontal = T, id = list(labels = rownames(df), n = 10), main = "Boxplot of PricePerSquareFoot")

abline(v=umout,col="orange",lwd=2) # vertical line for mild outliers
abline(v=usout,col="red",lwd=2.5) # vertical line for extreme outliers

abline(v=lmout,col="orange",lwd=2) # vertical line for mild outliers
abline(v=lsout,col="red",lwd=2.5) # vertical line for extreme outliers

mild_outliers_psqft <- which(df$pricePerSquareFoot < lmout | df$pricePerSquareFoot > umout)
extreme_outliers_psqft <- which(df$pricePerSquareFoot < lsout | df$pricePerSquareFoot > usout)

cat("Number of mild outliers:", length(mild_outliers_psqft), "\n")
cat("Number of extreme outliers:", length(extreme_outliers_psqft), "\n")
```

Also, the two most extreme outliers are removed from the dataset, as keeping them could introduce bias and negatively affect predictive model.

```{r}
# Delete 2 extreme outliers
df <- df[df$pricePerSquareFoot < 1000, ]
```


#### Variable `yearBuilt`

```{r}
# Summary statistics
summary(df$yearBuilt)
sum(is.na(df$yearBuilt))

# Histogram + Normal curve
hist(df$yearBuilt, breaks = 30, freq = FALSE, main = "Histogram of yearBuilt", xlab = "yearBuilt")
curve(dnorm(x, mean(df$yearBuilt, na.rm=TRUE), sd(df$yearBuilt, na.rm=TRUE)), add = TRUE, col = "red")
```
9999 represents a NA, we remove it as the year built likely cannot be imputated.

```{r}
df <- df[df$yearBuilt >= 1700 & df$yearBuilt <= 2025, ] # we only keep in the df the years within a realistic range
hist(df$yearBuilt, breaks = 30, freq = FALSE, main = "Histogram of yearBuilt", xlab = "yearBuilt")
curve(dnorm(x, mean(df$yearBuilt, na.rm=TRUE), sd(df$yearBuilt, na.rm=TRUE)), add = TRUE, col = "red")
```
The distribution of `yearBuilt` shows that most properties were constructed between 1950 and 2010, with a gradual increase in construction frequency over the decades and a peak around the early 2000s. Very few homes were built before 1900, indicating that such early values are rare and may represent historic properties.

```{r}
# Outliers

# Upper limits
varout <- summary(df$yearBuilt); varout
iqr <- varout[5] - varout[2]; iqr # diff between IQR = Q3 - Q1
umout <- varout[5] + 1.5*iqr; umout # upper limit to identify mild outliers
usout <- varout[5] + 3*iqr; usout # upper limit to identify extreme outliers

# Lower limits
lmout <- varout[2] - 1.5*iqr; umout # upper limit to identify mild outliers
lsout <- varout[2] - 3*iqr; usout # upper limit to identify extreme outliers

boxplot(df$yearBuilt, horizontal = T, id = list(labels = rownames(df), n = 10), main = "Boxplot yearBuilt")

abline(v=umout,col="orange",lwd=2) # vertical line for mild outliers
abline(v=usout,col="red",lwd=2.5) # vertical line for extreme outliers
abline(v=lmout,col="orange",lwd=2) # vertical line for mild outliers
abline(v=lsout,col="red",lwd=2.5) # vertical line for extreme outliers

mild_outliers_year <- which(df$yearBuilt < lmout | df$yearBuilt > umout)
extreme_outliers_year <- which(df$yearBuilt < lsout | df$yearBuilt > usout)

cat("Number of mild outliers:", length(mild_outliers_year), "\n")
cat("Number of extreme outliers:", length(extreme_outliers_year), "\n")
```
Although the IQR method identifies a set of mild and extreme outliers in `yearBuilt`, early construction years are possible, while very recent years reflect new constructions. Since these observations provide meaningful information about housing age, they are retained rather than removed or imputed.

```{r}
df$n.yearBuilt <- 2025 - df$yearBuilt  

# Summary statistics
summary(df$n.yearBuilt)
sum(is.na(df$n.yearBuilt))

# Histogram + Normal curve
hist(df$n.yearBuilt, breaks = 30, freq = FALSE, main = "Histogram of n.yearBuilt", xlab = "yearBuilt")
curve(dnorm(x, mean(df$n.yearBuilt, na.rm=TRUE), sd(df$n.yearBuilt, na.rm=TRUE)), add = TRUE, col = "red")
```

```{r}
# Outliers

# Upper limits
varout <- summary(df$n.yearBuilt); varout
iqr <- varout[5] - varout[2]; iqr # diff between IQR = Q3 - Q1
umout <- varout[5] + 1.5*iqr; umout # upper limit to identify mild outliers
usout <- varout[5] + 3*iqr; usout # upper limit to identify extreme outliers

# Lower limits
lmout <- varout[2] - 1.5*iqr; umout # upper limit to identify mild outliers
lsout <- varout[2] - 3*iqr; usout # upper limit to identify extreme outliers

boxplot(df$n.yearBuilt, horizontal = T, id = list(labels = rownames(df), n = 10), main = "Boxplot n.yearBuilt")

abline(v=umout,col="orange",lwd=2) # vertical line for mild outliers
abline(v=usout,col="red",lwd=2.5) # vertical line for extreme outliers
abline(v=lmout,col="orange",lwd=2) # vertical line for mild outliers
abline(v=lsout,col="red",lwd=2.5) # vertical line for extreme outliers

mild_outliers_nyear <- which(df$n.yearBuilt < lmout | df$n.yearBuilt > umout)
extreme_outliers_nyear <- which(df$n.yearBuilt < lsout | df$n.yearBuilt > usout)

cat("Number of mild outliers:", length(mild_outliers_nyear), "\n")
cat("Number of extreme outliers:", length(extreme_outliers_nyear), "\n")
```

#### Variable `livingArea`

```{r}
summary(df$livingArea)
sum(is.na(df$livingArea))
```

```{r}
# Histogram
hist(df$livingArea, breaks = 30, freq = F, main="Histogram of livingArea", xlab = "livingArea")
curve(dnorm(x, mean(df$livingArea, na.rm=TRUE), sd(df$livingArea, na.rm=TRUE)), add = TRUE, col = "red")
```
```{r}
# Outliers

# Upper limits
varout <- summary(df$livingArea); varout
iqr <- varout[5] - varout[2]; iqr # diff between IQR = Q3 - Q1
umout <- varout[5] + 1.5*iqr; umout # upper limit to identify mild outliers
usout <- varout[5] + 3*iqr; usout # upper limit to identify extreme outliers

# Lower limits
lmout <- varout[5] - 1.5*iqr; umout # upper limit to identify mild outliers
lsout <- varout[5] - 3*iqr; usout # upper limit to identify extreme outliers

boxplot(df$livingArea, horizontal = T, id = list(labels = rownames(df), n = 10), main = "Boxplot of livingArea")
abline(v=umout,col="orange",lwd=2) # vertical line for mild outliers
abline(v=usout,col="red",lwd=2.5) # vertical line for extreme outliers
abline(v=lmout,col="orange",lwd=2) # vertical line for mild outliers
abline(v=lsout,col="red",lwd=2.5) # vertical line for extreme outliers

mild_outliers_area <- which(df$livingArea < lmout | df$livingArea > umout)
extreme_outliers_area <- which(df$livingArea < lsout | df$livingArea > usout)

cat("Number of mild outliers:", length(mild_outliers_area), "\n")
cat("Number of extreme outliers:", length(extreme_outliers_area), "\n")
```
The `livingArea` variable shows a wide range of property sizes, with most homes falling between approximately 1,500 and 3,000 square feet. The IQR method identifies 247 mild outliers and 4 extreme outliers, which correspond to very large homes exceeding 5,000 sq ft. These values are likely not data errors but represent luxury properties with larger living spaces. Therefore, these outliers are retained, as they reflect meaningful variability in property size.


#### Variable `bathrooms`: num
Bathroom is a discrete numerical variable. 
```{r}
summary(df$bathrooms)
sum(is.na(df$bathrooms))
```

```{r}
# Histogram
hist(df$bathrooms, breaks = 30, freq = F, main="Histogram of bathrooms", xlab = "bathrooms")
curve(dnorm(x, mean(df$bathrooms, na.rm=TRUE), sd(df$bathrooms, na.rm=TRUE)), add = TRUE, col = "red")
```
As can be seen in the histogram, some observations have 0 bathrooms. Since a house cannot realistically have 0 bathrooms, these values are replaced with NA to indicate missing data and then later we will impute the variable using the **MICE** algorithm. 

```{r}
# Replace impossible values (0 bathrooms) with NA
df$bathrooms[df$bathrooms == 0] <- NA
# We check if it was applied correctly
sum(is.na(df$bathrooms))
```
After inspecting the advertisements for properties with 9 and 10 bathrooms, we confirmed that these values are accurate and correspond to luxury homes. Therefore, they are not treated as outliers, but retained as valid observations reflecting luxury properties in the data set.


#### Variable `bedrooms`: num
Bedrooms is a discrete numerical variable. 
```{r}
summary(df$bedrooms)
sum(is.na(df$bedrooms))
sum(df$bedrooms == 0)
```

```{r}
# Histogram
hist(df$bedrooms, breaks = 30, freq = F, main="Histogram of bedrooms", xlab = "bedrooms")
curve(dnorm(x, mean(df$bedrooms, na.rm=TRUE), sd(df$bedrooms, na.rm=TRUE)), add = TRUE, col = "red")
```

As can be seen in the histogram, some observations have 0 bedrooms. Just as `bathrooms` variable, these values are replaced with NA to indicate missing data and then later we will impute the variable using the **MICE** algorithm. 

```{r}
# Replace impossible values (0 bedrooms) with NA
df$bedrooms[df$bedrooms == 0] <- NA
# We check if it was applied correctly
sum(is.na(df$bedrooms))
```

### 2.1.3. Categorical variables

#### Variable `cityId`
```{r}
df$cityId <- as.factor(df$cityId)
summary(df$cityId)
```


```{r}
# Frequency table
table(df$cityId)

# Barplot
barplot(table(df$cityId), main = "Barplot of cityId", las = 2, col = "lightblue")
```
```{r}
print(sum(df$cityId == 0))
```
Variable `cityId` has 297 observations of value 0. 

```{r}
# count unique cityIds per city
library(dplyr)

df %>%
  group_by(city) %>%
  summarise(n_cityId = n_distinct(cityId)) %>%
  arrange(desc(n_cityId))
```

The `cityId` variable shows inconsistencies, with several cities associated with multiple different IDs (for example, Atlanta has 21 distinct codes). Since `cityId` is a redundant and unreliable encoding of the city information, it will be removed from the dataset. We can get this information with the variable `city`.


#### Variable `event`
```{r}
# Frequency table
table(df$event)

# Proportions
prop.table(table(df$event))

# Barplot
barplot(table(df$event), main = "Barplot of event", las = 2, col = "lightblue")
```
Since `event` primarily indicates listing status and not intrinsic property characteristics, it may not contribute significantly to price prediction. It's strongly imbalanced and it could be removed or simplified to a binary indicator (active vs. sold).

```{r}
df$event_status <- as.character(df$event)

df$event_status[df$event_status %in% c("Listed for sale", "Pending sale", "Price change", "Listing removed")] <- "Active"
df$event_status[df$event_status == "Sold"] <- "Sold"
df$event_status <- factor(df$event_status)
table(df$event_status)

```

Although the `event` variable was simplified into “Active” and “Sold,” the “Sold” category represents less than 1% of the dataset (49 out of 5465 observations). 

Due to this extreme imbalance and the fact that `event` status does not describe physical property characteristics, it is unlikely to provide meaningful predictive power and will therefore be removed from the modeling dataset.


#### Variable `city`
```{r}
df$city = factor(df$city)
table(df$city)
```
First, we observed that some city names were duplicated due to abbreviations, spelling variations, or inconsistent capitalization. To correct this, we converted city names to lowercase and merged the following cases:

Strange cases we observe:
* Union Point - Union Pt
* West Point - West Pt
* Avondale Est - Avondale Estates
* Waverly - Waverly Hall

Also, there are several cases of the same name with different capital letters:
* La fayette - La Fayette
* Rock spring - Rock Spring
* Rocky Face - Rocky face
* Talking Rock -  Talking rock
* Trenton - TRENTON
* Warner robins - Warner Robins
* Blue Ridge - Blue ridge
* Lagrange - LAGRANGE
* Johns Creek - JOHNS CREEK

```{r}
# Convert to lowercase and trim
df$city <- tolower(trimws(df$city))

df$city[df$city %in% c("union pt")] <- "union point"
df$city[df$city %in% c("west pt")] <- "west point"
df$city[df$city %in% c("avondale est")] <- "avondale estates"
df$city[df$city %in% c("waverly hall", "waverly")] <- "waverly hall"
df$city[df$city %in% c("la fayette", "la fayette")] <- "la fayette"
df$city[df$city %in% c("rock spring", "rock spring")] <- "rock spring"
df$city[df$city %in% c("rocky face", "rocky face")] <- "rocky face"
df$city[df$city %in% c("talking rock", "talking rock")] <- "talking rock"
df$city[df$city %in% c("lagrange", "lagrange", "lagrange")] <- "lagrange"
df$city[df$city %in% c("johns creek", "johns creek")] <- "johns creek"
df$city[df$city %in% c("warner robins", "warner robins")] <- "warner robins"
df$city[df$city %in% c("blue ridge", "blue ridge")] <- "blue ridge"

df$city <- factor(df$city)
table(df$city)
```
After these corrections, each city is represented by a single standardized level.

#### Variable `hasGarage`
```{r}
df$hasGarage = factor(df$hasGarage)
table(df$hasGarage)
prop.table(table(df$hasGarage))
barplot(table(df$hasGarage), main = "Barplot of hasGarage", col="lightblue")
sum(is.na(df$hasGarage))
```
Most properties in the dataset (approximately 63%) have a garage, while the remaining 37% do not. Since garage availability may influence property price and desirability, this variable will be retained.


#### Variable `parking`
```{r}
table(df$parking)
```
```{r}
df$parking = factor(df$parking)
barplot(table(df$parking), main = "Barplot of Parking", col="orange")
```
Same as with `hasGarage`, the parking availability could affect the property price and desirability, thus, we maintain the variable.

#### Variable `garageSpaces`
```{r}
table(df$garageSpaces)
```
```{r}
df$garageSpaces = factor(df$garageSpaces)
barplot(table(df$garageSpaces), main = "Barplot of garageSpaces", col="orange")
```
Since higher numbers of `garageSpaces` have fewer observations, we consider them as 1 level. 

```{r}
df$garageSpaces <- as.character(df$garageSpaces)
df$garageSpaces[df$garageSpaces>=3] <- "3 or more"
df$garageSpaces <- as.factor(df$garageSpaces)
table(df$garageSpaces)
```
```{r}
barplot(table(df$garageSpaces), main = "Barplot of garageSpaces", col="orange")
```

#### Variable `levels`
```{r}
df$levels = factor(df$levels)
table(df$levels)
barplot(table(df$levels), main = "Barplot of levels", col="pink")
sum(is.na(df$levels))
```
We evaluate the number of unique values in the strings to unify them. 
```{r}
unique(df$levels)
```
To simplify the levels variable, some in between levels like the "one and one half" level was reassigned based on its similarity in average price to other groups. After comparing mean prices across levels, it was grouped with "two" to reduce sparsity and improve model interpretability. The same happens with "zero", which was grouped with "one".

```{r}
boxplot(price ~ levels, data = df,
        main = "Price by Level",
        col = "lightblue",
        las = 2)
```

```{r}
# Standardize text formatting (lowercase + trim)
df$levels <- tolower(trimws(df$levels))

# Define categories
one_group <- c(
  "0", "one", "one-other", "one-manufactured home 1 story",
  "one-mobile home 1 story", "one-two story foyer", "one-two",
  "one-one and one half", "one-one and one half-two",
  "manufactured home 1 story", "mobile home 1 story"
)

two_group <- c(
  "one and one half", "one and one half-split level",
  "one and one half-multi/split", "1.5 story", "one and one half-two",
  "two", "two-split foyer", "two-split level", "two-two story foyer",
  "two-foyer - 2 story", "two-three or more", "multi/split-two",
  "two-multi/split"
)

three_group <- c(
  "three or more", "three or more-two story foyer",
  "3 story", "2.5 story", "tri level", "tri-level",
  "tri level-split level", "three or more-split level-tri-level",
  "multi/split", "multi/split-split level",
  "split level", "split foyer"
)

# Recode values
df$levels[df$levels %in% one_group] <- "one"
df$levels[df$levels %in% two_group] <- "two"
df$levels[df$levels %in% three_group] <- "three or more"

# Handle remaining categories (other-see remarks)
df$levels[!df$levels %in% c("one", "two", "three or more")] <- NA

# convert to factor
df$levels <- factor(df$levels, levels = c("one", "two", "three or more"))

# Check result
table(df$levels)
summary(df$levels)

```
```{r}
barplot(table(df$levels), main = "Barplot of levels", col="orange")
```


#### Variable `pool`
```{r}
df$pool = factor(df$pool)
table(df$pool)
barplot(table(df$pool), main = "Barplot of pool", col="lightblue")
sum(is.na(df$pool))
```

#### Variable `spa`
```{r}
df$spa = factor(df$spa)
table(df$spa)
barplot(table(df$spa), main = "Barplot of spa", col="lightblue")
sum(is.na(df$spa))
```

#### Variable `isNewConstruction`
```{r}
df$isNewConstruction = factor(df$isNewConstruction)
table(df$isNewConstruction)
barplot(table(df$isNewConstruction), main = "Barplot of isNewConstruction", col="lightblue")
sum(is.na(df$isNewConstruction))
```
The variables `pool`, `spa` and `isNewConstruction` may influence the house price, so we keep them.

#### Variable `hasPetsAllowed`
We see that there's very little houses with pets allowed 51 vs 6041, so we can consider it does not contribute to the model's prediction. We can remove it.
```{r}
df$hasPetsAllowed = factor(df$hasPetsAllowed)
table(df$hasPetsAllowed)
barplot(table(df$hasPetsAllowed), main = "Barplot of hasPetsAllowed", col="lightblue")
sum(is.na(df$hasPetsAllowed))
```

#### Variable `homeType`
```{r}
df$homeType = factor(df$homeType)
table(df$homeType)
barplot(table(df$homeType), main = "Barplot of homeType", col="lightblue")
sum(is.na(df$homeType))
```
Most properties are single-family homes, while townhouses, condos, and especially multi-family units are a minority. This imbalance should be considered in modeling. We can check whether the home type influences the price:

```{r}
boxplot(df$price ~ df$homeType, main="Price by Home Type", col="lightblue")
```
Multi-family units and condos are generally the cheaper, with lower median prices and limited price dispersion. Townhouses show a slightly higher median price compared to single-family homes, but single-family homes have a much wider price range and include numerous luxury properties.


#### Variable `f.price`: cat
We created an additional ordinal variable `f.price` by discretizing the continuous `Price` variable into four quartile-based categories: "Low", "Medium-Low", "Medium-High", and "High". 

```{r}
q <- quantile(df$price, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE)

df$f.price <- cut(df$price,
                  breaks = q,
                  include.lowest = TRUE,
                  labels = c("Low", "Medium-Low", "Medium-High", "High"))
table(df$f.price)

barplot(table(df$f.price), main = "Barplot of f.price", col="lightblue")
```

#### Variable `f.yearBuilt`: cat
We created an additional ordinal variable `f.yearBuilt` by discretizing `n.yearBuilt` variable into four new categories: "New", "Recent", "Old" and "Historic". 

```{r}
df$f.yearBuilt <- cut(df$n.yearBuilt,
                      breaks = c(-Inf, 10, 30, 100, Inf),
                      labels = c("New", "Recent", "Old", "Historic"),
                      right = TRUE)

table(df$f.yearBuilt)
barplot(table(df$f.yearBuilt), main = "Barplot of f.yearBuilt", col="lightblue")
```

## Removed variable analysis

In addition to the variables previously discussed, `countyId` and `cityId` will also be removed due to inconsistencies, such as multiple codes representing the same city, and `cityId` values equal to 0. 

As we said before, these variables are redundant, as they simply encode the categorical information already present in `county` and `city`, which are cleaner and more interpretable. 

Variables `hasPetsAllowed` and `event` will be removed because of the big class imbalance.

```{r}
# Columns removed after initial analysis
df2 <- df[,-c(1:12, 16, 18:24, 26, 29, 37)]
```

Finally, all the variables removed from the dataset were excluded due to low informational value, redundancy, or lack of variability. The following variables were discarded:

- X.1 (1)
- X (2)
- id (3)
- stateId (4)
- countyId (5)
- cityId (6)
- country (7)
- datePostedString (8)
- is_bankOwned (9)
- is_forAuction (10)
- event (11)
- time (12)
- state (16)
- streetAddress (18)
- zipcode (19)
- longitude (20)
- latitude (21)
- hasBadGeocode (22)
- description (23)
- currency (24)
- livingAreaValue (26)
- buildingArea (29)
- hasPetsAllowed (37)


The dataset obtained after the preprocessing contains the following variables:
- 7 numerical variables: `price`, `pricePerSquareFoot`, `yearBuilt`, `n.yearBuilt`, `livingArea`, `bathrooms`, `bedrooms`.
- 13 categorical variables: `city`, `state`, `parking`, `garageSpaces`, `hasGarage`, `levels`, `pool`, `spa`, `isNewConstruction`, `homeType`, `county`, `f.price`, `f.yearBuilt`.

```{r}
# Numerical variables:
dfnum <- df2[,c(1:2, 4:7)] # o yearBuilt o n.yearBuilt (escollir una)

# Categorical variables:
dfcat <- df2[,c(3, 8:20)]

summary(dfnum)
```
```{r}
summary(dfcat)
```

To explore potential linear dependencies among the numerical features, we computed their correlation matrix.

```{r}
require(corrplot)
par(mfrow=c(1,1)) 
corrplot(cor(dfnum, use = "complete.obs"), method = "number")
```

The correlation matrix shows that `price` is strongly associated with `livingArea` and `bathrooms`, while features such as `yearBuilt` and `garageSpaces` exhibit only weak correlations with the target variable. Additionally, some predictors (e.g., `livingArea`, `bathrooms`, and `bedrooms`) are highly correlated with each other, which may indicate potential multicollinearity.


## 2.2. Missing values
In the dataset there are missing values in variables:
- Bedrooms
- Bathrooms
- Levels

```{r}
summary(df2)
colSums(is.na(df2))
na_per_row <- rowSums(is.na(df2))
table(na_per_row)

df2[rowSums(is.na(df2)) > 1, ]

```
The problem is that in 11 observations, there is a missing value for both bedrooms and bathrooms.
Since they are correlated with price and each other (?), and they represent only a 0.2% of observations, we have decided to eliminate them, so as to not introduce too many errors.
!!!!!! Pongo tambien el codigo sin eliminarlas x si acso
```{r}
# --- Packages ---
library(mice)      # multiple imputation
library(lattice)   # densityplot / stripplot used by mice
# library(ggplot2) # optional, only if you want custom plots later

# --- 0) Make a working copy (keep your original df untouched) ---
df_work <- df2

# --- 1) (Optional) Remove rows where BOTH bedrooms and bathrooms are NA ---
#     Rationale: there are only ~11 such rows (~0.2%); removing them can simplify/strengthen imputation.
drop_both_na <- FALSE  # <- set to TRUE if you want to drop those 11 rows

if (drop_both_na) {
  both_na_idx <- is.na(df_work$bedrooms) & is.na(df_work$bathrooms)
  message("Dropping rows with bedrooms & bathrooms both NA: ", sum(both_na_idx))
  df_work <- df_work[!both_na_idx, ]
}

# --- 2) Ensure correct types (levels should be a factor) ---
if (!is.factor(df_work$levels)) {
  df_work$levels <- factor(df_work$levels)
}

# --- 3) Select variables for imputation + predictors (adjust as you like) ---
vars <- c(
  "bathrooms", "bedrooms", "levels",        # variables to impute
  "livingArea", "price", "pricePerSquareFoot",
  "garageSpaces", "hasGarage", "parking",
  "pool", "spa",
  "homeType", "isNewConstruction",
  "yearBuilt"                               # predictors only
)

# Subset to the columns used for MICE
df_mice <- df_work[vars]

# --- 4) Initialize mice to get default method and predictor matrix ---
ini  <- mice(df_mice, maxit = 0, printFlag = FALSE)
meth <- ini$method
pred <- ini$predictorMatrix

# --- 5) Set imputation methods ---
# Numeric discrete -> "pmm"; Categorical (factor) -> "polyreg"
meth["bathrooms"] <- "pmm"
meth["bedrooms"]  <- "pmm"
meth["levels"]    <- "polyreg"

# Optional: do NOT impute any other variable (predictors only)
to_leave <- c("bathrooms", "bedrooms", "levels")
meth[setdiff(names(meth), to_leave)] <- ""

# (Optional) You can also tweak the predictor matrix if needed, e.g.:
# pred["bathrooms", c("bathrooms")] <- 0  # do not self-predict (mice already handles this)
# pred["bedrooms",  c("bedrooms")]  <- 0
# pred["levels",    c("levels")]    <- 0
# If you found convergence issues before, consider removing very-high-cardinality factors from pred.

# --- 6) Run MICE (impute all three at once) ---
# m = 5 imputations is usually fine; increase to 10 for extra stability.
set.seed(123)
imp <- mice(
  df_mice,
  method = meth,
  predictorMatrix = pred,
  m = 5,
  maxit = 5,          # increase to 10 if you want more iterations
  printFlag = TRUE
)

# --- 7) Quick diagnostics ---
print(imp)                  # summary of methods and # of imputations
md.pattern(df_mice)         # missingness pattern (before imputation)
densityplot(imp, ~ bathrooms + bedrooms)  # check distributions
# For the factor 'levels', compare counts before vs after:
cat("Levels (before):\n"); print(table(df_mice$levels, useNA = "ifany"))
df_check <- complete(imp, 1)
cat("Levels (after, imp #1):\n"); print(table(df_check$levels))

# --- 8) (Optional) Inspect the exact imputed values ---
# Shows the imputed cells only (rows that were NA originally)
imp$imp$bathrooms  # each column = one imputed dataset
imp$imp$bedrooms
imp$imp$levels

# --- 9) Create one completed dataset (choose any 1..m, or pool later if needed) ---
df_completed <- complete(imp, action = 1)

# --- 10) Replace ONLY the missing cells back into your original df ---
# Bathrooms
na_bath_idx <- is.na(df_work$bathrooms)
df_work$bathrooms[na_bath_idx] <- df_completed$bathrooms[na_bath_idx]

# Bedrooms
na_bed_idx <- is.na(df_work$bedrooms)
df_work$bedrooms[na_bed_idx] <- df_completed$bedrooms[na_bed_idx]

# Levels
na_levels_idx <- is.na(df_work$levels)
df_work$levels[na_levels_idx] <- df_completed$levels[na_levels_idx]

# --- 11) (Optional) Keep a flag of which values were imputed (use imp$where for robustness) ---
# Note: imp$where indicates which cells were NA in df_mice at the time of mice()
imputed_flags <- as.data.frame(imp$where[, c("bathrooms","bedrooms","levels")])
names(imputed_flags) <- c("imp_bathrooms", "imp_bedrooms", "imp_levels")
# Bind to df_work if you want to keep the flags for downstream checks
df_work <- cbind(df_work, imputed_flags)

# --- 12) Final checks ---
summary(df_work$bathrooms)
summary(df_work$bedrooms)
table(df_work$levels)

# df_work now contains the imputed values (and flags, if you kept them).
# If you decide to drop the 11 rows, set drop_both_na = TRUE at the top.

summary(df_work)

```


!!!!!!!!Este codigo es de prueba (el que habia hecho solo con bathrooms)
```{r}
library(mice)
library(ggplot2)

# Seleccionamos las variables relevantes
vars <- c("bathrooms", "bedrooms", "livingArea", "levels", 
          "price",
          "garageSpaces", "hasGarage", "parking", 
          "pool", "spa",
          "homeType", "isNewConstruction",
          "yearBuilt")

# Subconjunto
df_sub <- df[vars]

# Configuramos MICE
ini <- mice(df_sub, maxit = 0)
meth <- ini$method
pred <- ini$predictorMatrix

# Especificamos método solo para bathrooms
meth["bathrooms"] <- "pmm"

# Opcional: imputar solo bathrooms (las demás se ignoran)
meth[setdiff(names(meth), "bathrooms")] <- ""

# Ejecutamos imputación
imp <- mice(df_sub, method = meth, predictorMatrix = pred, m = 5, seed = 123)

# Dataset completo
df_imputed <- complete(imp)

# Comprobar que tenga sentido 
densityplot(imp, ~ bathrooms)

library(ggplot2)

library(ggplot2)

# 1) Saca un dataset completo (p.ej., la 1ª imputación)
df_check <- complete(imp, action = 1)

# 2) Flag robusto de "esto fue NA antes de imputar"
#    (usa EXACTAMENTE el nombre de la variable imputada en el objeto mice)
df_check$.imp_flag <- imp$where[, "bathrooms"]

# (opcional) comprueba cuántos hay
cat("Imputed points:", sum(df_check$.imp_flag), "\n")

# 3) Dibuja primero observados en gris y luego imputados en rojo encima
ggplot() +
  geom_point(
    data = df_check[!df_check$.imp_flag, ],
    aes(x = livingArea, y = bathrooms),
    color = "grey60", alpha = 0.5, size = 1.7
  ) +
  geom_point(
    data = df_check[df_check$.imp_flag, ],
    aes(x = livingArea, y = bathrooms),
    color = "tomato", alpha = 1, size = 3
  ) +
  geom_smooth(
    data = df_check,
    aes(x = livingArea, y = bathrooms),
    method = "lm", se = FALSE, color = "black"
  ) +
  labs(
    title = "Bathrooms vs Living Area (Observed vs Imputed)",
    x = "Living Area (sq ft)", y = "Bathrooms"
  ) +
  theme_minimal(base_size = 13)
-.
summary(lm(bathrooms ~ livingArea + bedrooms + price + garageSpaces + levels, data = df_check))


# Reemplazamos bathrooms imputados en el df original
df$bathrooms[is.na(df$bathrooms)] <- df_imputed$bathrooms[is.na(df$bathrooms)]

```


```{r}
summary(df2)
# Imputation: MICE
library(mice)
# Method 'pmm' is used for numerical variables
res.imp <- mice(df2, method = 'pmm', seed=123)
df$bathrooms <- complete(res.imp, 1)$bathrooms
summary(df$bathrooms)
```


## 2.3. Detection of outliers
### 2.3.1. Univariate outliers
We have managed some of the outliers in the previous section, the ones we have not removed are taken into account here:
```{r}

# all rows that have an extreme outlier for some variable
total_extreme_outliers <- unique(c(
  extreme_outliers_price,
  extreme_outliers_psqft,
  extreme_outliers_year,
  extreme_outliers_area
))

total_mild_outliers <- unique(c(
  mild_outliers_price,
  mild_outliers_psqft,
  mild_outliers_year,
  mild_outliers_area
))

```


### 2.3.2. Multivariate outliers
To identify observations that behave unusually when considering all numerical variables together, we applied a multivariate outlier analysis using Mahalanobis distance. This method detects data points that stand out from the general pattern, even if they do not appear extreme in individual variables. The flagged observations were then further inspected as potential multivariate outliers.

```{r}
library(chemometrics) 

res.mout <- Moutlier(dfnum,quantile=0.995, plot=F) 
str(res.mout) # list of: 1. md: Mahalanobis distance / 2. rd / 3. cutoff (threshold)

par(mfrow=c(1,1))
plot(res.mout$md,res.mout$rd,col="cyan",pch=19)
res.mout$cutoff
abline(h=res.mout$cutoff,col="red",lwd=2)
abline(v=res.mout$cutoff,col="red",lwd=2)
text(res.mout$md,res.mout$rd,label=row.names(df),cex=0.5) # we can see that physicians and general man are the identified outliers (>cutoff)

ll <- which((res.mout$md>res.mout$cutoff) & (res.mout$rd>res.mout$cutoff)); ll # observations with md and rd > cutoff

moutliers <- length(ll)/length(df2)
moutliers
```


# 3. Address tests to discard serial correlation

```{r}
acf(df2$price) 
```

The ACF plot of the variable price shows significant positive autocorrelation at the first lags, indicating that nearby observations are not independent. Therefore, serial correlation cannot be discarded, and the temporal structure of the data should be taken into account in the modelling process.

Since the temporal order of the observations is not relevant for the predictive model and autocorrelation was observed in the target variable, the dataset was randomly shuffled to remove artificial serial dependence.

```{r}
ll <- sample(1:nrow(df), nrow(df))
df3 <- df2[ll, ]
acf(df3$price) 
```


